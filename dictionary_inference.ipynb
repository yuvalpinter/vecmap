{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "\n",
    "from embeddings import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse-engineering language dictionaries from synsets\n",
    "\n",
    "This notebook will serve (and later be replaced with a script) for extracting a word-level dictionary from synset alignment tables, where the English table is given and the target-language (starting with Italian) is learned as part of the large iterative process.\n",
    "\n",
    "Our first step is to re-extract the English synset dictionary in its reduced form for the top 20,000 vocabulary items, as in `map_sense_embeddings.py`. This includes a later removal of resulting empty synset columns.\n",
    "\n",
    "Note that we don't have to re-align the synsets with their identifiers; they're just serving as a vessel for dictionary alignment for now.\n",
    "\n",
    "The git commit this code is copied from is [`00c662b`](https://github.com/yuvalpinter/vecmap/commit/00c662b740198e428f5e82d936f4af93f90e1ffa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60167, 200000) 80296\n",
      "trimmed sense dictionary dimensions: (20000, 35784) with 44795 nonzeros\n"
     ]
    }
   ],
   "source": [
    "# load source alignment, trim\n",
    "src_size = 20000\n",
    "src_sns_filename = 'data/synsets/v3b_pairings.pkl'\n",
    "\n",
    "with open(src_sns_filename, 'rb') as src_sns_file:\n",
    "    src_senses = pickle.load(src_sns_file)\n",
    "\n",
    "print(src_senses.shape, src_senses.getnnz())  # should be (60167, 200000) 80296\n",
    "if src_senses.shape[0] < 100000:  # We want words as rows\n",
    "    src_senses = src_senses.transpose()\n",
    "src_senses = src_senses[:src_size]\n",
    "\n",
    "# new columns for words with no senses in original input\n",
    "newcols = [sparse.csc_matrix(([1],([i],[0])), shape=(src_size, 1)) for i in range(src_size)\\\n",
    "                   if src_senses.getrow(i).getnnz() == 0]\n",
    "# trim senses no longer used, add new ones\n",
    "colsums = src_senses.sum(axis=0).tolist()[0]\n",
    "src_senses = sparse.hstack([src_senses[:,[i for i,j in enumerate(colsums) if j>0]]] + newcols).tocsr()\n",
    "\n",
    "# this should be (20000, 35784) with 44795 nonzeros\n",
    "print(f'trimmed sense dictionary dimensions: {src_senses.shape} with {src_senses.getnnz()} nonzeros')\n",
    "sense_size = src_senses.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curvature, daytona, correspondents, radiology, persians, saffron, catalonia, skipton, cola, xl\n",
      "minerva, irresponsabile, anzianita&apos;, rilassante, qualificanti, sgomento, deontologia, immaginiamo, gianluigi, lieti\n"
     ]
    }
   ],
   "source": [
    "# load word lists\n",
    "\n",
    "encod = 'utf-8'\n",
    "src_embs_file = 'data/embeddings/en.emb.txt'  # can also use en-words.txt\n",
    "trg_embs_file = 'data/embeddings/it.emb.txt'\n",
    "\n",
    "with open(src_embs_file, encoding=encod, errors='surrogateescape') as src_embs:\n",
    "    src_words = read(src_embs, threshold=src_size)[0][:src_size]\n",
    "    \n",
    "with open(trg_embs_file, encoding=encod, errors='surrogateescape') as trg_embs:\n",
    "    trg_words = read(trg_embs, threshold=src_size)[0][:src_size]\n",
    "    \n",
    "print(', '.join(src_words[-10:]))\n",
    "print(', '.join(trg_words[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the target assignments. We'll start with a small table that only assigned a few dozen senses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First iteration alignment has shape (20000, 35784) with 18 nonzeros.\n"
     ]
    }
   ],
   "source": [
    "trg_sns_filename = lambda n: f'outputs/tsns-00c662b-it{n:03d}.pkl'\n",
    "\n",
    "with open(trg_sns_filename(1), 'rb') as trg_sense_1iter_file:\n",
    "    trg_senses_1iter = pickle.load(trg_sense_1iter_file)\n",
    "    \n",
    "print(f'First iteration alignment has shape {trg_senses_1iter.shape} with {trg_senses_1iter.getnnz()} nonzeros.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a sense of which words got assigned senses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culturali, decretolegge, sedi, venti, porte, vitale, illustrato, richard, foggia, tasti, c.p., shock, camino, marittimi, fuso, iniziazione, gori\n"
     ]
    }
   ],
   "source": [
    "def assigned_trg_words(table, vocab=trg_words):\n",
    "    return [w for l, w in zip(table, vocab) if l.getnnz() > 0]\n",
    "\n",
    "print(', '.join(assigned_trg_words(trg_senses_1iter, trg_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooookaaaay.\n",
    "Which English words (senses) did they happen to align with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawns       foggia       0.031\n",
      "palette     sedi         0.022\n",
      "purchase    shock        0.022\n",
      "ads         culturali    0.018\n",
      "alison      iniziazione  0.017\n",
      "co-operate  gori         0.014\n",
      "penelope    illustrato   0.013\n",
      "sect        decretolegge 0.012\n",
      "postcode    porte        0.011\n",
      "robertson   richard      0.008\n",
      "districts   tasti        0.008\n",
      "bang        fuso         0.008\n",
      "delicious   camino       0.006\n",
      "kitchen     marittimi    0.006\n",
      "buzz        vitale       0.005\n",
      "vascular    c.p.         0.003\n",
      "belmont     venti        0.003\n"
     ]
    }
   ],
   "source": [
    "def find_alignments(src_tab, trg_tab, src_vocab=src_words, trg_vocab=trg_words, threshold=0.0):\n",
    "    alignments = []\n",
    "    common_synsets = sparse.coo_matrix(src_tab.dot(trg_tab.transpose()), copy=True)\n",
    "    for i,j,d in zip(common_synsets.row, common_synsets.col, common_synsets.data):\n",
    "        if d > threshold:\n",
    "            alignments.append((src_vocab[i], trg_vocab[j], d))\n",
    "    return sorted(alignments, key=lambda x: -x[-1])  # order by descending match scores\n",
    "\n",
    "iter1_align = find_alignments(src_senses, trg_senses_1iter)\n",
    "print('\\n'.join(['{: <12}{: <12} {:.3f}'.format(*a) for a in iter1_align]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34646]\n",
      "[34646]\n",
      "\n",
      "[13282 17786 17902 17903 17904 17905 17906 17907]\n",
      "[17902]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "lawns_idx = [i for i,w in enumerate(src_words) if w=='lawns'][0]\n",
    "foggia_idx = [i for i,w in enumerate(trg_words) if w=='foggia'][0]\n",
    "print(src_senses[lawns_idx].indices)\n",
    "print(trg_senses_1iter[foggia_idx].indices)\n",
    "\n",
    "print()\n",
    "\n",
    "bang_idx = [i for i,w in enumerate(src_words) if w=='bang'][0]\n",
    "fuso_idx = [i for i,w in enumerate(trg_words) if w=='fuso'][0]\n",
    "print(src_senses[bang_idx].indices)\n",
    "print(trg_senses_1iter[fuso_idx].indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One positive finding is that the first iteration was pretty conservative in its alignment scores.\n",
    "\n",
    "The words themselves seem pretty random except for maybe some POS agreement but hey, it's the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second iteration alignment has shape (20000, 35784) with 882 nonzeros.\n",
      "\n",
      "co-operate  gori         0.695\n",
      "lawns       foggia       0.694\n",
      "robertson   richard      0.691\n",
      "alison      iniziazione  0.690\n",
      "ads         culturali    0.690\n",
      "palette     sedi         0.689\n",
      "purchase    shock        0.688\n",
      "penelope    illustrato   0.686\n",
      "bang        fuso         0.685\n",
      "districts   tasti        0.683\n",
      "sect        decretolegge 0.682\n",
      "belmont     venti        0.680\n",
      "postcode    porte        0.678\n",
      "buzz        vitale       0.669\n",
      "kitchen     marittimi    0.665\n",
      "vascular    c.p.         0.664\n",
      "delicious   camino       0.646\n",
      "belmont     dieci        0.433\n",
      "belmont     trenta       0.428\n",
      "belmont     quindici     0.413\n"
     ]
    }
   ],
   "source": [
    "with open(trg_sns_filename(2), 'rb') as trg_sense_2iter_file:\n",
    "    trg_senses_2iter = pickle.load(trg_sense_2iter_file)\n",
    "    \n",
    "print(f'Second iteration alignment has shape {trg_senses_2iter.shape} with {trg_senses_2iter.getnnz()} nonzeros.\\n')\n",
    "\n",
    "iter2_align = find_alignments(src_senses, trg_senses_2iter)\n",
    "print('\\n'.join(['{: <12}{: <12} {:.3f}'.format(*a) for a in iter2_align][:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher confidence, lots of new alignments, some of the old ones self-fed, healthy overall. Let's jump straight to the last iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourth iteration alignment has shape (20000, 35784) with 59943 nonzeros.\n",
      "\n",
      "bang          fuso           0.618\n",
      "wards         alt            0.600\n",
      "waverley      arca           0.600\n",
      "denise        ascensione     0.582\n",
      "outlines      incisione      0.574\n",
      "bosch         supplementare  0.568\n",
      "kitchens      rotte          0.567\n",
      "buildings     borsa          0.557\n",
      "anderson      gordon         0.549\n",
      "destinations  legittimi      0.545\n",
      "temperate     circondario    0.539\n",
      "townships     leve           0.537\n",
      "cosmetic      classificazione 0.537\n",
      "biographies   sottile        0.535\n",
      "garden        crotone        0.525\n",
      "grading       lit            0.524\n",
      "tasty         casale         0.522\n",
      "foliage       caltanissetta  0.521\n",
      "sentiments    decisione      0.519\n",
      "seafood       canna          0.517\n"
     ]
    }
   ],
   "source": [
    "with open(trg_sns_filename(4), 'rb') as trg_sense_4iter_file:\n",
    "    trg_senses_4iter = pickle.load(trg_sense_4iter_file)\n",
    "    \n",
    "print(f'Fourth iteration alignment has shape {trg_senses_4iter.shape} with {trg_senses_4iter.getnnz()} nonzeros.\\n')\n",
    "\n",
    "iter4_align = find_alignments(src_senses, trg_senses_4iter, threshold=0.5)\n",
    "print('\\n'.join(['{: <14}{: <14} {:.3f}'.format(*a) for a in iter4_align][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fifth iteration alignment has shape (20000, 35784) with 79018 nonzeros.\n",
      "\n",
      "bang          fuso           0.600\n",
      "wards         alt            0.588\n",
      "waverley      arca           0.582\n",
      "biographies   sottile        0.580\n",
      "denise        ascensione     0.565\n",
      "greatness     datata         0.542\n",
      "greetings     paternità      0.539\n",
      "hazel         tormentato     0.537\n",
      "feelings      rinuncia       0.535\n",
      "anderson      gordon         0.534\n",
      "diary         trame          0.533\n",
      "garden        crotone        0.531\n",
      "brad          chiamiamo      0.529\n",
      "passport      ragionevolmente 0.529\n",
      "nicola        danza          0.528\n",
      "kitchens      rotte          0.528\n",
      "celebrities   ordinata       0.528\n",
      "culinary      frazione       0.518\n",
      "counsellors   prostituzione  0.515\n",
      "stores        mercati        0.515\n"
     ]
    }
   ],
   "source": [
    "with open(trg_sns_filename(5), 'rb') as trg_sense_5iter_file:\n",
    "    trg_senses_5iter = pickle.load(trg_sense_5iter_file)\n",
    "    \n",
    "print(f'Fifth iteration alignment has shape {trg_senses_5iter.shape} with {trg_senses_5iter.getnnz()} nonzeros.\\n')\n",
    "\n",
    "iter5_align = find_alignments(src_senses, trg_senses_5iter, threshold=0.5)\n",
    "print('\\n'.join(['{: <14}{: <14} {:.3f}'.format(*a) for a in iter5_align][:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's a lot of Italian sense assignments, let's see what are the top polysemic words, if any exist at all.\n",
    "We'll threshold the top two entries and sort by the best second-ranked mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 20000/20000 [00:03<00:00, 6585.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd   total   word   word\n",
      "score senses  index\n",
      "0.261   2     9295   dea\n",
      "0.178   2     1941   ospedale\n",
      "0.166   6     5002   avanzate\n",
      "0.142   3     16776  centrocampista\n",
      "0.136   2     8464   113\n",
      "0.130   5     4928   situato\n",
      "0.130   4     7017   juve\n",
      "0.129   3     11091  sudan\n",
      "0.128   4     15316  usiamo\n",
      "0.123   3     14069  decorazione\n"
     ]
    }
   ],
   "source": [
    "topk = 10\n",
    "\n",
    "def push_cap(h, val):\n",
    "    if len(h) < topk:\n",
    "        heapq.heappush(h, val)\n",
    "    else:\n",
    "        _ = heapq.heappushpop(h, val)\n",
    "\n",
    "kheap = []\n",
    "for j in tqdm(range(src_size)):\n",
    "    row = trg_senses_5iter[j]\n",
    "    if row.getnnz() > 1:\n",
    "        second_val = sorted(row.data, reverse=True)[1]\n",
    "        push_cap(kheap, (second_val, trg_senses_5iter[j].getnnz(), j, trg_words[j]))\n",
    "\n",
    "print('2nd   total   word   word')\n",
    "print('score senses  index')\n",
    "print('\\n'.join(['{:.3f}   {: <3d}   {: <6d} {}'.format(*a) for a in sorted(kheap, key=lambda x: -x[0])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welp, can't say anything too valuable about these, or about the confidence of polysemy mapping.\n",
    "\n",
    "The top three and bottom three do seem polysemic, but certainly there are more prominent specimens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
