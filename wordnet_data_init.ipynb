{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning WordNet to an Embedding Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First item of business, getting the embedding dictionary word list (so we know which words we want to map to synsets to begin with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200001it [00:15, 12617.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10æ¥ä»¥å ã«çºé 0.039924000000000001 0.10034999999999999 0.042516999999999999\n",
      " the -0.098361000000000004 0.014168999999999999 0.039489999999999997\n",
      "7æ¥ä»¥å ã«çºé 0.019684 0.059399 -0.0065339999999999999\n"
     ]
    }
   ],
   "source": [
    "emb_tab_file = 'data/embeddings/en.emb.txt'\n",
    "en_words = []\n",
    "errors = []\n",
    "with open(emb_tab_file, 'rb') as embs:\n",
    "    for e in tqdm(embs):\n",
    "        cols = e.decode(\"utf-8\").split()\n",
    "        if len(cols) < 3: continue\n",
    "        try:\n",
    "            float(cols[1])\n",
    "        except ValueError:\n",
    "            errors.append(' '.join(cols[:5]))\n",
    "            continue\n",
    "        en_words.append(cols[0])\n",
    "print('\\n'.join(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'and', 'to', 'in', 'a', 'is', '\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hameroff', 'gaultheria', 'margriet', 'katzen', '10-credit']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_words[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's whip out WordNet and find alignments from synset to word(s).\n",
    "\n",
    "Our first attempt is just using the WordNet interface's `synsets()` function, which is string-based. All synsets found for each word are mapped to it in `pairings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199997it [00:20, 9764.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 186978 pairings for 71558 synsets and 199997 words.\n",
      "no wn records found for 128945 words.\n",
      "no embeddings found for 46101 synsets.\n"
     ]
    }
   ],
   "source": [
    "synsets = {}  # list of synsets, in order found by traversing embedding vocabl list\n",
    "pairings = []  # list of 1-valued cells in matrix to be built\n",
    "no_syns_found = 0  # counter for words with no synsets attached\n",
    "for i,w in tqdm(enumerate(en_words)):\n",
    "    syns = wn.synsets(w)\n",
    "    if len(syns) == 0:\n",
    "        no_syns_found += 1\n",
    "        continue\n",
    "    for sn in syns:\n",
    "        if sn not in synsets:\n",
    "            synsets[sn] = len(synsets)\n",
    "        pairings.append((synsets[sn], i))\n",
    "itosyns = {i:sn for sn,i in synsets.items()}\n",
    "no_word = len([sn for sn in wn.all_synsets() if sn not in synsets])\n",
    "print(f'found {len(pairings)} pairings for {len(synsets)} synsets and {len(en_words)} words.')\n",
    "print(f'no wn records found for {no_syns_found} words.')\n",
    "print(f'no embeddings found for {no_word} synsets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check. The word *in* might be a good example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('inch.n.01'),\n",
       " Synset('indium.n.01'),\n",
       " Synset('indiana.n.01'),\n",
       " Synset('in.s.01'),\n",
       " Synset('in.s.02'),\n",
       " Synset('in.s.03'),\n",
       " Synset('in.r.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_sns = [itosyns[j] for j in [p[0] for p in pairings if p[1] == en_words.index('in')]]\n",
    "in_sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a problem. We don't really want these mappings for the clearly-prepositional word 'in'.\n",
    "Maybe we should trim by lemma count? Case in point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(Lemma('inch.n.01.inch'), 49), (Lemma('inch.n.01.in'), 3)],\n",
       " [(Lemma('indium.n.01.indium'), 0),\n",
       "  (Lemma('indium.n.01.In'), 0),\n",
       "  (Lemma('indium.n.01.atomic_number_49'), 0)],\n",
       " [(Lemma('indiana.n.01.Indiana'), 2),\n",
       "  (Lemma('indiana.n.01.Hoosier_State'), 0),\n",
       "  (Lemma('indiana.n.01.IN'), 0)],\n",
       " [(Lemma('in.s.01.in'), 0)],\n",
       " [(Lemma('in.s.02.in'), 0)],\n",
       " [(Lemma('in.s.03.in'), 0)],\n",
       " [(Lemma('in.r.01.in'), 19),\n",
       "  (Lemma('in.r.01.inwards'), 0),\n",
       "  (Lemma('in.r.01.inward'), 0)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(l, l.count()) for l in sn.lemmas()] for sn in in_sns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. New strategy. Only link to synset if the corresponding lemma's count is nonzero, see where that gets us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199997it [00:47, 4183.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 29560 pairings for 23154 synsets and 199997 words.\n",
      "no wn records found for 184018 words.\n",
      "no embeddings found for 94505 synsets.\n"
     ]
    }
   ],
   "source": [
    "trimmed_synsets = {}  # list of synsets, in order found by traversing embedding vocabl list\n",
    "idx_to_trimmed_synsets = {}  # inverted index\n",
    "trimmed_pairings = []  # list of 1-valued cells in matrix to be built\n",
    "no_trim_syns_found = 0  # counter for words with no synsets attached\n",
    "for i,w in tqdm(enumerate(en_words)):\n",
    "    lemmata = wn.lemmas(w)\n",
    "    syns = [lm.synset() for lm in lemmata if lm.count() > 0]\n",
    "    if len(syns) == 0:\n",
    "        no_trim_syns_found += 1\n",
    "        continue\n",
    "    for sn in syns:\n",
    "        if sn not in trimmed_synsets:\n",
    "            j = len(trimmed_synsets)\n",
    "            trimmed_synsets[sn] = j\n",
    "            idx_to_trimmed_synsets[j] = sn\n",
    "        trimmed_pairings.append((trimmed_synsets[sn], i))        \n",
    "no_trim_word = len([sn for sn in wn.all_synsets() if sn not in trimmed_synsets])\n",
    "print(f'found {len(trimmed_pairings)} pairings for {len(trimmed_synsets)} synsets and {len(en_words)} words.')\n",
    "print(f'no wn records found for {no_trim_syns_found} words.')\n",
    "print(f'no embeddings found for {no_trim_word} synsets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's some heavy trimming. We only have ~15k words with matches.\n",
    "Although now it's only ~2 pairs per word, not ~3, I think we can add one more heuristic:\n",
    "It's ok to pair a word with a synset if it's the only one corresponding to it, or if it's the only lemma in the synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199997it [00:39, 5016.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 80623 pairings for 60167 synsets and 199997 words.\n",
      "no wn records found for 155851 words.\n",
      "no embeddings found for 57492 synsets.\n"
     ]
    }
   ],
   "source": [
    "v3_synsets = {}  # list of synsets, in order found by traversing embedding vocabl list\n",
    "idx_to_v3_synsets = {}  # inverted index\n",
    "v3_pairings = []  # list of 1-valued cells in matrix to be built\n",
    "no_v3_syns_found = 0  # counter for words with no synsets attached\n",
    "for i,w in tqdm(enumerate(en_words)):\n",
    "    lemmata = wn.lemmas(w)\n",
    "    if len(lemmata) == 1:\n",
    "        syns = wn.synsets(w)\n",
    "    else:\n",
    "        syns = [lm.synset() for lm in lemmata if lm.count() > 0 or len(lm.synset().lemmas()) == 1]\n",
    "    if len(syns) == 0:\n",
    "        no_v3_syns_found += 1\n",
    "        continue\n",
    "    for sn in syns:\n",
    "        if sn not in v3_synsets:\n",
    "            j = len(v3_synsets)\n",
    "            v3_synsets[sn] = j\n",
    "            idx_to_v3_synsets[j] = sn\n",
    "        v3_pairings.append((v3_synsets[sn], i))        \n",
    "no_v3_word = len([sn for sn in wn.all_synsets() if sn not in v3_synsets])\n",
    "print(f'found {len(v3_pairings)} pairings for {len(v3_synsets)} synsets and {len(en_words)} words.')\n",
    "print(f'no wn records found for {no_v3_syns_found} words.')\n",
    "print(f'no embeddings found for {no_v3_word} synsets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super-sanity check\n",
    "for j in range(len(v3_synsets)):\n",
    "    assert j in idx_to_v3_synsets, j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substantially better. 3 times as many vocab words found in WN, 2.6x as many pairings.\n",
    "\n",
    "(Note: without the `or`-clause it was 2.5x vocab words, 2x pairings. This clause brings rows with a single entry, is this something we want? It's a concept that exists and might have a different word in the target language, although it might be too rare, something like an obscure meaning of *have*.)\n",
    "\n",
    "TODO: what about **inflections** in word embedding vocab? Do we pair with the same synset(s)?\n",
    "We probably shouldn't, since inflections don't carry across languages, but in this case what do we do with those words?\n",
    "\n",
    "Another TODO is to retrain the embedding model to include ngram phrases that we know about in WN (i.e. preprocess the upstream corpus). Even if it is only done in English, presumably concepts in the target language might be more frequently single words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Graph Properties\n",
    "\n",
    "Let's create a sparse graph object for simple querying and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pairings = sorted(v3_pairings)\n",
    "indices = []\n",
    "indptr = []\n",
    "x = -1\n",
    "for idx,(j,i) in enumerate(sorted_pairings):\n",
    "    if j != x:\n",
    "        indptr.append(idx)\n",
    "        x = j\n",
    "    indices.append(i)\n",
    "indptr.append(idx)\n",
    "data = [1] * (idx+1)\n",
    "\n",
    "pairings_graph = csr_matrix((data,indices,indptr), shape=(sorted_pairings[-1][0]+1, max([i for j,i in v3_pairings])+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctness check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60167, 199994)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairings_graph.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0]),\n",
       " array([    40,     79,    114,    835,   3296, 126060]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairings_graph[56].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('merely.r.01.merely'),\n",
       " Lemma('merely.r.01.simply'),\n",
       " Lemma('merely.r.01.just'),\n",
       " Lemma('merely.r.01.only'),\n",
       " Lemma('merely.r.01.but')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_v3_synsets[56].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('but', 'just', 'merely', 'but')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_words[40], en_words[114], en_words[3296], en_words[126060]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hrmph. That second *but* has some unicode thing `U+0085 NEXT LINE CHARACTER` attached to it that was removed when processing the embeddings file.\n",
    "\n",
    "TODO get rid of these, there's 77 of them, 55 of whom at word ends, who knows what other characters may be lurking :-/\n",
    "\n",
    "Now let's talk degree distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1,\n",
       "         1: 47524,\n",
       "         2: 8465,\n",
       "         3: 2404,\n",
       "         4: 899,\n",
       "         5: 415,\n",
       "         6: 215,\n",
       "         7: 115,\n",
       "         8: 62,\n",
       "         9: 32,\n",
       "         10: 15,\n",
       "         11: 8,\n",
       "         12: 4,\n",
       "         13: 1,\n",
       "         14: 3,\n",
       "         15: 3,\n",
       "         20: 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words per synset\n",
    "x_degs = pairings_graph.sum(axis=1).flatten().tolist()[0]\n",
    "xdeg_counts = Counter(x_degs)\n",
    "xdeg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: check who that '0' is and remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 155849,\n",
       "         1: 30499,\n",
       "         2: 6453,\n",
       "         3: 2908,\n",
       "         4: 1430,\n",
       "         5: 938,\n",
       "         6: 514,\n",
       "         7: 401,\n",
       "         8: 264,\n",
       "         9: 156,\n",
       "         10: 121,\n",
       "         11: 84,\n",
       "         12: 84,\n",
       "         13: 52,\n",
       "         14: 43,\n",
       "         15: 38,\n",
       "         16: 29,\n",
       "         17: 13,\n",
       "         18: 21,\n",
       "         19: 18,\n",
       "         20: 9,\n",
       "         21: 6,\n",
       "         22: 7,\n",
       "         23: 8,\n",
       "         24: 2,\n",
       "         25: 2,\n",
       "         26: 6,\n",
       "         27: 2,\n",
       "         28: 3,\n",
       "         29: 7,\n",
       "         30: 2,\n",
       "         31: 4,\n",
       "         32: 1,\n",
       "         33: 1,\n",
       "         34: 3,\n",
       "         35: 1,\n",
       "         36: 1,\n",
       "         37: 4,\n",
       "         38: 1,\n",
       "         39: 1,\n",
       "         43: 2,\n",
       "         44: 2,\n",
       "         45: 1,\n",
       "         56: 1,\n",
       "         60: 2})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# synsets per word\n",
    "y_degs = pairings_graph.sum(axis=0).flatten().tolist()[0]\n",
    "ydeg_counts = Counter(y_degs)\n",
    "ydeg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually looks pretty sane.\n",
    "\n",
    "We'll write the matrix and index dictionaries to files for use in the algorithm. Synsets don't have to be qualified by name for the currently-designed algorithm, but we'll keep the name-mapping anyway, by list of their string names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to files\n",
    "if False:  # change when actually writing\n",
    "    synset_list = [sn.name() for _,sn in sorted(idx_to_v3_synsets.items(), key = lambda x: x[0])]\n",
    "    with open('data/v3a_synsets.txt', 'w') as synset_list_file:\n",
    "        for sn in synset_list:\n",
    "            synset_list_file.write(f'{sn}\\n')\n",
    "\n",
    "    with open('data/v3a_wordlist.txt', 'w', encoding='utf8') as wordlist_file:\n",
    "        for w in en_words:\n",
    "            wordlist_file.write(f'{w}\\n')\n",
    "\n",
    "    with open('data/v3a_pairings.pkl', 'wb') as graph_file:\n",
    "        pickle.dump(pairings_graph, graph_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inducing WN Structure on Limited Dataset\n",
    "\n",
    "Let's continue with the structural properties of our induced WordNet (for phase II of the project).\n",
    "\n",
    "I'll copy over some things from [the M3GM notebook](https://github.com/yuvalpinter/m3gm/blob/master/wn_exploration/wordnet_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total synsets: 60,167\n",
      "32494 have hypernyms\n",
      " 9411 have hyponyms\n",
      "  311 have entailments\n",
      "  707 have attributes\n",
      "  180 have causes\n",
      "  639 have member meronyms\n",
      "  200 have substance meronyms\n",
      " 1342 have part meronyms\n",
      " 1013 have member holonyms\n",
      "  175 have substance holonyms\n",
      " 2936 have part holonyms\n",
      " 8600 have similar tos\n"
     ]
    }
   ],
   "source": [
    "def nonempty_rels_count(rel, dset=v3_synsets) -> int:\n",
    "    \"\"\"\n",
    "    Computes out degree\n",
    "    \"\"\"\n",
    "    return len([s for s in dset if len([t for t in rel(s) if t in dset]) > 0])\n",
    "\n",
    "print(f'Total synsets: {len(v3_synsets):,}')\n",
    "rel_names = ['hypernyms', 'hyponyms', 'entailments', 'attributes', 'causes', 'member meronyms', \\\n",
    "             'substance meronyms', 'part meronyms', 'member holonyms', 'substance holonyms', 'part holonyms', 'similar tos']\n",
    "rel_funcs = [lambda s: s.hypernyms(), lambda s: s.hyponyms(), lambda s: s.entailments(), lambda s: s.attributes(),\\\n",
    "                      lambda s: s.causes(), lambda s: s.member_meronyms(), lambda s: s.substance_meronyms(),\\\n",
    "                      lambda s: s.part_meronyms(), lambda s: s.member_holonyms(), lambda s: s.substance_holonyms(), \\\n",
    "                      lambda s: s.part_holonyms(), lambda s: s.similar_tos()]\n",
    "\n",
    "for name, rel in zip(rel_names, rel_funcs):\n",
    "    print(f'{nonempty_rels_count(rel):5d} have {name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graphify(relation) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a networkx directed graph out of a WordNet synset relation type\n",
    "    \"\"\"\n",
    "    relg = nx.DiGraph()\n",
    "    relg.add_nodes_from(v3_synsets)\n",
    "    for s in v3_synsets:\n",
    "        targets = [t for t in relation(s) if t in v3_synsets]\n",
    "        relg.add_edges_from(zip([s] * len(targets), targets))\n",
    "    return relg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypg = graphify(lambda s: s.hyponyms())\n",
    "hypg.remove_edge(wn.synset('inhibit.v.04'), wn.synset('restrain.v.01'))\n",
    "nx.dag.is_directed_acyclic_graph(hypg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyponym graph roots is 1963\n",
      "Roots with largest subgraphs:\n",
      "organism.n.01 : 2,771\n",
      "causal_agent.n.01 : 2,764\n",
      "artifact.n.01 : 2,721\n",
      "event.n.01 : 2,346\n",
      "attribute.n.02 : 2,004\n",
      "matter.n.03 : 1,085\n",
      "change.v.01 : 1,059\n",
      "relation.n.01 : 946\n",
      "cognition.n.01 : 871\n",
      "move.v.02 : 817\n"
     ]
    }
   ],
   "source": [
    "in_out_degs = {n:(i,o) for ((n,i),(n,o)) in zip(hypg.in_degree(), hypg.out_degree())}\n",
    "roots = [n for n,(i,o) in in_out_degs.items() if i == 0 and o > 0]\n",
    "\n",
    "print(f'Number of hyponym graph roots is {len(roots)}')\n",
    "subtrees = {r:len(nx.dfs_tree(hypg, r)) for r in roots}\n",
    "print('Roots with largest subgraphs:')\n",
    "for x in sorted(subtrees.items(), key=lambda x:-x[1])[:10]:\n",
    "    print(f'{x[0].name()} : {x[1]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more roots than we're used to. Most oddly of all, where's `entity.n.01`? (TODO)\n",
    "\n",
    "Other than that, the big ones seem to be there.\n",
    "\n",
    "The following are snippets from the M3GM network with results copied over as line comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act has 771 hyponyms, Move has 817\n",
      "They have 0 hyponyms in common\n"
     ]
    }
   ],
   "source": [
    "act_root = wn.synset('act.v.01')\n",
    "move_root = wn.synset('move.v.02')\n",
    "act_st = nx.dfs_tree(hypg, act_root)\n",
    "move_st = nx.dfs_tree(hypg, move_root)\n",
    "print(f'Act has {len(act_st):,} hyponyms, Move has {len(move_st):,}') # verification\n",
    "print(f'They have {len([s for s in act_st if s in move_st])} hyponyms in common') # intersection (not necessarily 0, since this is a DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-child fraction for Act is 0.11673\n",
      "Single-child fraction for Move is 0.10526\n"
     ]
    }
   ],
   "source": [
    "def sc_frac(tree):\n",
    "    \"\"\"\n",
    "    Returns the fraction of nodes in a tree that only have one child\n",
    "    \"\"\"\n",
    "    return len([d for _,d in tree.out_degree() if d == 1]) / len(tree)\n",
    "\n",
    "print(f'Single-child fraction for Act is {sc_frac(act_st):.5f}')  # around 0.11 in full graph\n",
    "print(f'Single-child fraction for Move is {sc_frac(move_st):.5f}')  # around 0.11 in full graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median depth for Act is 4\n",
      "Median depth for Move is 3\n"
     ]
    }
   ],
   "source": [
    "def all_depths(tree, root):\n",
    "    return [nx.shortest_path_length(tree, root, n) for n in tree]\n",
    "\n",
    "def med_depth(tree, root):\n",
    "    return int(np.median(all_depths(tree, root)))\n",
    "\n",
    "print(f'Median depth for Act is {med_depth(act_st, act_root)}')  # 5 in full graph\n",
    "print(f'Median depth for Move is {med_depth(move_st, move_root)}')  # 3 in full graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width for Act tree is 174\n",
      "Width for Move tree is 252\n"
     ]
    }
   ],
   "source": [
    "def tree_width(tree, root):\n",
    "    return Counter(all_depths(tree, root)).most_common(1)[0][1]\n",
    "\n",
    "print(f'Width for Act tree is {tree_width(act_st, act_root)}')  # 226 in full graph\n",
    "print(f'Width for Move tree is {tree_width(move_st, move_root)}')  # 311 in full graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyponym connections between parts of speech:\n",
      "n -> n: 23,306 times\n",
      "v -> v: 9,541 times\n"
     ]
    }
   ],
   "source": [
    "def pos_connected(e, directed=True) -> tuple:\n",
    "    \"\"\"\n",
    "    A function telling us which parts-of-speech are participating in an edge\n",
    "    :param e: an nx edge\n",
    "    :param directed: True iff we care about which node comes first (not so for polysemy)\n",
    "    \"\"\"\n",
    "    if not directed:\n",
    "        return tuple(sorted([e[0].pos(), e[1].pos()]))\n",
    "    return (e[0].pos(), e[1].pos())\n",
    "\n",
    "def pos_connections(graph: nx.Graph, directed=True) -> Counter:\n",
    "    \"\"\"\n",
    "    Counts part-of-speech relations in a graph.\n",
    "    \"\"\"\n",
    "    return Counter([pos_connected(e, directed) for e in graph.edges()])\n",
    "\n",
    "def pos_connection_viz(graph):\n",
    "    print('\\n'.join([f'{p1} -> {p2}: {ct:,} times' for (p1,p2),ct in pos_connections(graph).most_common()]))\n",
    "\n",
    "print('Hyponym connections between parts of speech:')\n",
    "pos_connection_viz(hypg)  # in original graph: 75,850 n -> n, 13,238 v -> v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning that the verb synsets were better-preserved, which makes sense due to the longer-tailedness of the noun concept domain.\n",
    "\n",
    "All-in-all, this appears to be a reasonable graph that can be useful as an embedding projection signal, and may be used in an ERGM estimation protocol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
