{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Toy Dataset for Translingual Semantic Induction\n",
    "\n",
    "In this notebook, we will create a dataset of 100 words with 20-dimensional embeddings which are a subset of the 200k, 300d English-Italian dataset, that will be used as a tight-loop test-bed for the synset-alignment idea.\n",
    "\n",
    "We'll try to take an interesting subset in a non-random manner, such that the various components may be sanity-checked (although not necessarily improve results). This means enough English words will have Italian correspondences, and enough nontrivial parts of the WordNet graph (including polysemy) will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read(file, maxlines=10000, dim=20, threshold=0, vocabulary=None, dtype='float'):\n",
    "    header = file.readline().decode(\"utf-8\").split(' ')\n",
    "    words = []\n",
    "    matrix = np.empty((maxlines, dim), dtype=dtype) if vocabulary is None else []\n",
    "    for i in range(maxlines):\n",
    "        word, vec = file.readline().decode(\"utf-8\").split(' ', 1)\n",
    "        if vocabulary is None:\n",
    "            words.append(word)\n",
    "            matrix[i] = np.fromstring(vec, sep=' ', count=dim, dtype=dtype)\n",
    "        elif word in vocabulary:\n",
    "            words.append(word)\n",
    "            matrix.append(np.fromstring(vec, sep=' ', count=dim, dtype=dtype))\n",
    "    return (words, matrix) if vocabulary is None else (words, np.array(matrix, dtype=dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/embeddings/en.emb.txt', 'rb') as enembs:\n",
    "    testing = read(enembs, maxlines=100, dim=5, vocabulary=['in','the','house'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the', 'in'], array([[ 0.025749, -0.006358, -0.001263,  0.092149,  0.084755],\n",
       "        [ 0.024942,  0.008304, -0.033839,  0.078987,  0.135708]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good.\n",
    "\n",
    "Here's what we'll do now. We'll take the first 1,000 vectors in English and see how many Italian word equivalents we've found. If it's not close enough to 100, we'll iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/embeddings/en.emb.txt', 'rb') as enembs:\n",
    "    k_words, k_vecs = read(enembs, maxlines=1000, dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', ':', \"'s\", 'are', 'at']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_words[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_it_dict = {}\n",
    "for dictfile in ['data/dictionaries/en-it.train.txt', 'data/dictionaries/en-it.test.txt']:\n",
    "    with open(dictfile) as dfile:\n",
    "        for l in dfile:\n",
    "            en, it = l.strip().split()\n",
    "            en_it_dict[en] = it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'casa'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_it_dict['house']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([w for w in k_words if w in en_it_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrific! Let's get a random subsample, hoping for a similar % of translateable words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 90210  # donna martin on my mind\n",
    "rand_idcs = np.random.choice(range(1000), size=100)\n",
    "d_words = [w for i,w in enumerate(k_words) if i in rand_idcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yet', 'return', 'among', 'million', 'story']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_words[40:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([w for w in d_words if w in en_it_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Let's fish for multisense words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "word_in_synset_table_idcs = {}\n",
    "with open('data/synsets/v3a_wordlist.txt') as wordfile:\n",
    "    for i in range(1000):\n",
    "        w = wordfile.readline().strip()\n",
    "        if w in d_words:\n",
    "            word_in_synset_table_idcs[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_in_synset_table_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backup_d_words = d_words\n",
    "d_words = list(word_in_synset_table_idcs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_synset_pairing = pickle.load(open('data/synsets/v3a_pairings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words correspond to columns\n",
    "full_syn_columns = full_synset_pairing[:, np.array(list(word_in_synset_table_idcs.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "populated_rows = [r for r in full_syn_columns[:] if r.nnz > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(populated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't good - the big dataset has 60K synsets for 200K words, we want to keep the ratio sensible. Let's truncate the alignment arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_populated_rows = [i*5 for i,r in enumerate(full_syn_columns[::5]) if r.nnz > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_populated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_pairing = full_syn_columns[np.array(sampled_populated_rows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<82x94 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 84 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 80), (2, 2)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words per synset\n",
    "x_degs = partial_pairing.sum(axis=1).flatten().tolist()[0]\n",
    "xdeg_counts = Counter(x_degs)\n",
    "xdeg_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 42), (1, 33), (2, 10), (3, 6), (4, 2), (5, 1)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# synsets per word\n",
    "y_degs = partial_pairing.sum(axis=0).flatten().tolist()[0]\n",
    "ydeg_counts = Counter(y_degs)\n",
    "ydeg_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. So not much polysemy to go around, but some degree of multisense ambiguity. We can live with that.\n",
    "\n",
    "## Write to files\n",
    "Let's start with the semantic side. I don't think we need the synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/synsets/toy_wordlist.txt', 'w', encoding='utf8') as wordlist_file:\n",
    "    for w in d_words:\n",
    "        wordlist_file.write(f'{w}\\n')\n",
    "\n",
    "with open('data/synsets/toy_pairings.pkl', 'wb') as graph_file:\n",
    "    pickle.dump(partial_pairing, graph_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries can stay the same, since we're not doing seeded alignment anyway (for now), and if we ever will it'll only start with a seed of 25 words.\n",
    "\n",
    "Now for English embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "d_vecs = [v for i,v in enumerate(k_vecs) if i in rand_idcs and k_words[i] in d_words]\n",
    "print(len(d_vecs))  # should be 94\n",
    "with open('data/embeddings/en.toy.txt', 'w', encoding='utf8') as en_embs_file:\n",
    "    en_embs_file.write(f'{len(d_words)} {len(d_vecs[0])}\\n')\n",
    "    for w,v in zip(d_words, d_vecs):\n",
    "        en_embs_file.write(w + ' ' + ' '.join([f'{vd:.6f}' for vd in v]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for Italian. First we need to find the actual words for the ones we have, then randomly select a bunch of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it_d_words = [en_it_dict[w] for w in d_words if w in en_it_dict]\n",
    "len(it_d_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/embeddings/it.emb.txt', 'rb') as itembs:\n",
    "    it_words, it_vecs = read(itembs, maxlines=52000)\n",
    "len([w for w in it_words if w in it_d_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 5500\n",
    "add_words = np.random.choice([w for w in it_words[:1000] if w not in it_d_words], size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_d_words = [w for w in it_d_words if w in it_words] + add_words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "it_d_ordered_words_vecs = [(w, it_vecs[i]) for i,w in enumerate(it_words) if w in it_d_words]\n",
    "print(len(it_d_ordered_words_vecs))  # should be 95\n",
    "with open('data/embeddings/it.toy.txt', 'w', encoding='utf8') as it_embs_file:\n",
    "    it_embs_file.write(f'{len(it_d_ordered_words_vecs)} {len(it_d_ordered_words_vecs[0][1])}\\n')\n",
    "    for w,v in it_d_ordered_words_vecs:\n",
    "        it_embs_file.write(w + ' ' + ' '.join([f'{vd:.6f}' for vd in v]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, I think we're done?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
